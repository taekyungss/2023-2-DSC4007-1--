import torch
import gradio as gr
import time
import os
import tempfile
from pathlib import Path
from datetime import datetime

from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from transformers.utils import is_flash_attn_2_available
from transformers.pipelines.audio_utils import ffmpeg_read

from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage

# llm
from langchain import PromptTemplate, LLMChain
from langchain.llms import OpenAI
import pandas as pd

import os
import gradio as gr
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("API_KEY")

'''---------------------- STT ----------------------'''
BATCH_SIZE = 16
MAX_AUDIO_MINS = 30  # maximum audio input in minutes

device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
use_flash_attention_2 = is_flash_attn_2_available()

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "openai/whisper-large-v3", torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True,
    use_flash_attention_2=use_flash_attention_2
)

if not use_flash_attention_2:
    # use flash attention from pytorch sdpa
    model = model.to_bettertransformer()

processor = AutoProcessor.from_pretrained("openai/whisper-large-v3")

model.to(device)

pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    max_new_tokens=128,
    chunk_length_s=30,
    torch_dtype=torch_dtype,
    device=device,
    generate_kwargs={"language": "ko", "task": "transcribe"},
    return_timestamps=True
)
pipe_forward = pipe._forward


def transcribe(inputs):
    if inputs is None:
        raise gr.Error("ì…ë ¥ëœ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! ìš”ì²­ì„ í•˜ê¸° ì „ì— ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë…¹ìŒí•˜ê±°ë‚˜ ì—…ë¡œë“œí•˜ì‹­ì‹œì˜¤.")

    with open(inputs, "rb") as f:
        inputs = f.read()

    inputs = ffmpeg_read(inputs, pipe.feature_extractor.sampling_rate)
    audio_length_mins = len(inputs) / pipe.feature_extractor.sampling_rate / 60

    if audio_length_mins > MAX_AUDIO_MINS:
        raise gr.Error(
            f"í—ˆë½ëœ ìµœëŒ€ ì˜¤ë””ì˜¤ ê¸¸ì´ëŠ” {MAX_AUDIO_MINS} ë¶„ ì…ë‹ˆë‹¤."
            f"ì…ë ¥ëœ ì˜¤ë””ì˜¤ ê¸¸ì´ëŠ” {round(audio_length_mins, 3)} ë¶„ ì…ë‹ˆë‹¤"
        )

    inputs = {"array": inputs, "sampling_rate": pipe.feature_extractor.sampling_rate}

    def _forward_time(*args, **kwargs):
        global runtime
        start_time = time.time()
        result = pipe_forward(*args, **kwargs)
        runtime = time.time() - start_time
        runtime = round(runtime, 2)
        return result

    pipe._forward = _forward_time
    text = pipe(inputs, batch_size=BATCH_SIZE)["text"]

    yield text, runtime

'''---------------------- LLM ----------------------'''

data = pd.read_csv("../../../capstone-23_2/data.csv")
chat_template = f"""í•„ìˆ˜ : ë‚˜ëŠ” ë³µì§€ìƒë‹´ì„ ì§„í–‰í•˜ëŠ” ìŒì„±ë´‡ ì´ë©° ì´ë¦„ì€ ì•„ì´ì´ë‹¤.
í•œë²ˆì˜ ë‹µë³€ì— í•œê°€ì§€ ì§ˆë¬¸ë§Œ í•œë‹¤. 
ê³µì†í•˜ê³  ì˜ˆì˜ë°”ë¥´ê²Œ ë§ì„ í•´ì•¼í•œë‹¤. 
ì „í™”í†µí™”ë¥¼ í•˜ëŠ” ê²ƒì²˜ëŸ¼ ëŒ€í™”ë¥¼ í•´ì•¼í•œë‹¤. 
ë‹µë³€ì€ 2ì¤„ì´ë‚´ë¡œ ì§„í–‰í•œë‹¤. 
í•´ë‹¹ ì „í™”ëŠ” ë³¸ ì±—ë´‡ì´ ë¨¼ì € ì „í™”ë¥¼ ê±´ ìƒí™©ì´ë‹¤.
ì´ 6ë²ˆì´ìƒì˜ ëŒ€í™”ê°€ ì˜¤ê³ ê°€ë„ë¡ í•œë‹¤

0. ìƒëŒ€ë°©ì˜ ì´ë¦„ì€ {data["ì´ë¦„"][1]}ì´ë‹¤. í•´ë‹¹ ì´ë¦„ìœ¼ë¡œ ë¶€ë¥´ë©° ëŒ€í™”ë¥¼ í•œë‹¤.

1. ì²«ë²ˆì§¸ ë‹µë³€ì€ ì•ˆë…•í•˜ì„¸ìš” 000ë‹˜, ì§€ê¸ˆ í˜„ì¬ {data["ë¬¸ì œ"][1]}ë¡œ í™•ì¸ì´ ë˜ëŠ”ë° ì–´ë ¤ì›€ì€ ì—†ëŠ”ì§€ ì§ˆë¬¸í•œë‹¤.

2. {data["ë¬¸ì œ"][1]}ì´ ê±´ê°•ë¬¸ì œë¼ë©´ ì–´ë”” ì•„í”„ì‹ ê³³ì´ ìˆëƒê³  ì§ˆë¬¸í•œë‹¤.

3. {data["ë¬¸ì œ"][1]}ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ìƒí™©ì€ ì–´ë– í•œì§€ ì§ˆë¬¸ì„ í•œë‹¤.

4. ìƒëŒ€ë°©ì˜ ë¬¸ì œì— ëŒ€í•´ ì–´ëŠì •ë„ ì§ˆë¬¸ì„ í–ˆìœ¼ë©´ ì£¼ë³€ì— ë„ì›€ì„ ìš”ì²­í• ë§Œí•œ ë³´í˜¸ìê°€ ì—†ëŠ”ì§€ ë¬¼ì–´ë³¸ë‹¤.

5. ì§ˆë¬¸ì´ ìµœì†Œ 5ë²ˆ ì´ìƒ ì˜¤ê°€ê³  ë‚œ ì´í›„ì—, ìƒëŒ€ë°©ì˜ ì–´ë ¤ì›€ì— ê´€ë ¨ëœ ë³µì§€ ì„œë¹„ìŠ¤ë¥¼ ì¶”ì²œí•´ì¤€ë‹¤.


"""

chat_ai  = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'], temperature=0.5, model="gpt-4-1106-preview")

def response(message, history):  ## human, ai ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ìœ¼ë¡œ ì§€ì •
    # ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤ë¥¼ ì…ë ¥í•˜ë©´ ìµœì¢… ìš”ì•½ ì¶œë ¥
    if message == "ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤":
        return summarize(history)

    history_langchain_format = []
    # í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    history_langchain_format.append(SystemMessage(content=chat_template))
    # ì´ì „ ëŒ€í™” ê¸°ì–µ
    for human, ai in history:
        history_langchain_format.append(HumanMessage(content=human))
        history_langchain_format.append(AIMessage(content=ai))
    history_langchain_format.append(HumanMessage(content=message))
    print(history_langchain_format)
    gpt_response = chat_ai(history_langchain_format)
    return gpt_response.content, [[message,gpt_response.content]]


# ìš”ì•½ llm ëª¨ë¸ ì¶”ê°€
# summarize_llmê³¼ historyë¡œ ì¶œë ¥
def summarize(history):
    for i, (human, ai) in enumerate(history):
        history[i] = (f"ì‚¬ìš©ì : {human}", f"AI : {ai}")

    summarize_llm = OpenAI(
        temperature=0.3, model="gpt-3.5-turbo-instruct", max_tokens=512
    )


    summarize_template = """
    í•„ìˆ˜ : AIì™€ ì‚¬ìš©ì ëª¨ë‘ì˜ ëŒ€í™”ë¥¼ í•©ì³ì„œ ìš”ì•½í• ê±°ì•¼.
    0. ì‚¬ìš©ìì˜ ë¬¸ì œìƒí™©ì´ ë­”ì§€ ì •í™•í•˜ê²Œ íŒŒì•…í•˜ê³  ìš”ì•½í•´ì•¼í•´.

    {texts} ëŒ€í™”ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ë¬¸ ì‘ì„±í•´ì¤˜.
    """

    summarize_prompt = PromptTemplate(
        template=summarize_template, input_variables=["texts"]
    )
    summarize_chain = LLMChain(prompt=summarize_prompt, llm=summarize_llm)

    return summarize_chain.run(history)




'''---------------------- TTS ----------------------'''

client = OpenAI()
def tts(text, model, voice):
    response = client.audio.speech.create(
        model=model,  # "tts-1","tts-1-hd"
        voice=voice,  # 'alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'
        input=text,
    )


    speech_file_path = Path(__file__).parent / f"{datetime.now().strftime('%Y-%m-%d_%H_%M_%S')}.mp3"

    print(speech_file_path)
    start_time = time.time()
    response.stream_to_file(speech_file_path)
    runtime = time.time() - start_time

    runtime = round(runtime, 2)

    yield speech_file_path, runtime


if __name__ == "__main__":
    with gr.Blocks() as demo:
        with gr.Column():
            gr.HTML(
                """
                    <div style="text-align: center; max-width: 700px; margin: 0 auto;">
                      <div style="display: inline-flex; align-items: center; gap: 0.8rem; font-size: 1.75rem; " >
                        <h1 style="font-weight: 900; margin-bottom: 7px; line-height: normal; style: "> ì·¨ì•½ê³„ì¸µì„ ìœ„í•œ AIìŒì„±ì±—ë´‡  </h1>
                      </div>
                    </div>
                """
            )
            gr.HTML(
                f"""
                <ol> <STT>  openAI: Whisper-large-v2 </ol>
                """
            )
            #STT
            audio = gr.components.Audio(type="filepath", label="ìŒì„± ì…ë ¥")
            button = gr.Button("ì „ì‚¬í•˜ê¸°")
            with gr.Row():
                runtime = gr.components.Textbox(visible=False, label="Whisper ì „ì‚¬ì‹œê°„(s)")
            with gr.Row():
                transcription = gr.components.Textbox(label="Whisper ì „ì‚¬ë‚´ìš©", show_copy_button=True)
            button.click(
                    fn=transcribe,
                    inputs=audio,
                    outputs=[transcription, runtime],
                )
        with gr.Column():
            gr.HTML(
                f"""
                              <ol> <TTS> openAI: chatGPT-4 </ol>
                              """
            )
            with gr.Row():
                chatbot = gr.Chatbot()

            send = gr.Button("AI ëŒ€ë‹µë°›ê¸°")
            chatbot_text = gr.components.Textbox(visible=False)
            send.click(response, [transcription, chatbot], outputs=[chatbot_text,chatbot])

        '''
            gr.ChatInterface(
                fn=response,
                textbox=gr.Textbox(placeholder="ë§ê±¸ì–´ì£¼ì„¸ìš”..", container=False, scale=7),
                # ì±„íŒ…ì°½ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•œë‹¤.
                chatbot=gr.Chatbot(height=500),
                title="ì•„ì´(ìœ™ìœ™ì´)",
                description='ëŒ€í™”ê°€ ëë‚˜ë©´ "ëŒ€í™”ì¢…ë£Œ" ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”',
                theme="soft",
                retry_btn="ë‹¤ì‹œë³´ë‚´ê¸° â†©",
                undo_btn="ì´ì „ì±— ì‚­ì œ âŒ",
                clear_btn="ì „ì±— ì‚­ì œ ğŸ’«",
            )
        '''
        with gr.Column():
            gr.HTML(
                f"""
                      <ol> <TTS>  openAI: TTS-1 </ol>
                      """
            )
            #TTS
            with gr.Row():
                model = gr.Dropdown(choices=['tts-1', 'tts-1-hd'], label='ëª¨ë¸', value='tts-1')
                voice = gr.Dropdown(choices=['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'], label='ë³´ì´ìŠ¤ì˜µì…˜',
                                    value='nova')
            #text = gr.Textbox(label="í…ìŠ¤íŠ¸ì…ë ¥",
                           #   placeholder="í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ Enter í‚¤ë¥¼ ëˆ„ë¦…ë‹ˆë‹¤.")
            btn = gr.Button("ìŒì„±í•©ì„±í•˜ê¸°")
            output_audio = gr.Audio(label="ìŒì„± ì¶œë ¥")
            print(chatbot)
            chatbot_text.submit(fn=tts, inputs=[chatbot_text, model, voice], outputs=[output_audio,runtime], api_name="tts")
            btn.click(fn=tts, inputs=[chatbot_text, model, voice], outputs=[output_audio,runtime], api_name="tts")

        #ìƒ˜í”Œ
        gr.Markdown("## ìŒì„±ìƒ˜í”Œ")
        gr.Examples(
            [["task1_01.wav"], ["task1_02.wav"]],
            audio,
            outputs=[transcription, runtime],
            fn=transcribe,
            cache_examples=False,
        )

    demo.queue(max_size=10).launch()